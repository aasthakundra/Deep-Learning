{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d979de",
   "metadata": {},
   "source": [
    "Name - Aastha Kundra\n",
    "\n",
    "PGID - 12310022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2ebf2",
   "metadata": {},
   "source": [
    "### Part 1: Understanding Neural Network Forward Propagation and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792d448",
   "metadata": {},
   "source": [
    "#### 1. Implement forward propagation (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef405f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.28\n",
      "0.82\n",
      "Output: 0.2519187741734451\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Given initial weights\n",
    "W1 = 0.10\n",
    "W2 = 0.27\n",
    "W3 = 0.11\n",
    "W4 = 0.15\n",
    "W5 = 0.18\n",
    "W6 = 0.16\n",
    "\n",
    "# Given inputs\n",
    "i1 = 2\n",
    "i2 = 4\n",
    "\n",
    "# Forward propagation\n",
    "h1 = i1 * W1 + i2 * W2\n",
    "h2 = i1 * W3 + i2 * W4\n",
    "print(h1)\n",
    "print(h2)\n",
    "\n",
    "# Activation function (assuming sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "a1 = sigmoid(h1)\n",
    "a2 = sigmoid(h2)\n",
    "\n",
    "out = a1 * W5 + a2 * W6\n",
    "print(\"Output:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e59c1",
   "metadata": {},
   "source": [
    "#### 2.Derivation for Backpropagation (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a769f5",
   "metadata": {},
   "source": [
    "Let's denote:\n",
    "\n",
    "i1, i2 as the inputs <br>\n",
    "w1, w2, w3, w4 as weights to the hidden layer while w5 and w6 as weights to the output layer <br>\n",
    "h1, h2 are the hidden layer before activations <br>\n",
    "a1, a2 are the hidden layer activations\n",
    "\n",
    "o as the output of the network <br>\n",
    "t as the true output <br>\n",
    "ϕ as the learning rate <br>\n",
    "MSE as the mean squared error loss function<br>\n",
    "b1, b2, and b3 are the biases for the hidden and output neurons, respectively\n",
    "\n",
    "\n",
    "Now, let's go through the steps of backpropagation to update the weights.\n",
    " \n",
    "**1. Forward Pass:**\n",
    "\n",
    "h1 = w1.i1 + w2.i2 + b1 <br>\n",
    "h2 = w3.i1 + w4.i2 + b2 <br>\n",
    "\n",
    "a1 = f(h1) = f(w1.i1 + w2.i2 + b1) <br>\n",
    "a2 = f(h2) = f(w3.i1 + w4.i2 + b2)\n",
    "\n",
    "o = w5.a1 + w6.a2 + b3\n",
    "\n",
    "**2. Error Calculation:**\n",
    "\n",
    "The mean squared error loss function is: MSE= 1/2*(o−t)^2\n",
    "\n",
    "**3. Backward Pass:**\n",
    "\n",
    "\n",
    "The gradients for the output layer weights W5 and W6 can be derived as follows:\n",
    "\n",
    "∂MSE/∂W5 = ∂MSE/∂o* ∂o/∂W5 =(o−t)⋅a1 <br>\n",
    "∂MSE/∂W6 = ∂MSE/∂o* ∂o/∂W6 =(o−t)⋅a2 <br>\n",
    "\n",
    "Similarly, the gradients for the hidden layer weights W1,W2,W3, and W4 can be derived using chain rule and the sigmoid activation function derivative:\n",
    "\n",
    "∂MSE/∂W1 = ∂MSE/∂a1* ∂a1/∂W1 = ∂MSE/∂o* ∂o/∂a1* ∂a1/∂W1 =  (o-t).w5.f'(h1).i1 <br>\n",
    "∂MSE/∂W2 = ∂MSE/∂a1* ∂a1/∂W2 = ∂MSE/∂o* ∂o/∂a1* ∂a1/∂W2 =  (o−t)⋅W5⋅f'(h1)⋅i2 <br>\n",
    "∂MSE/∂W3 = ∂MSE/∂a2* ∂a2/∂W3 = ∂MSE/∂o* ∂o/∂a2* ∂a2/∂W3 =  (o−t)⋅W6⋅f'(h2)⋅i1 <br>\n",
    "∂MSE/∂W4 = ∂MSE/∂a2* ∂a2/∂W4 = ∂MSE/∂o* ∂o/∂a2* ∂a2/∂W4 =  (o−t)⋅W6⋅f'(h2)⋅i2 <br>\n",
    "\n",
    "**4. Update weights:**\n",
    "\n",
    "w1 = w1 - ϕ* ∂MSE/∂W1 = w1 - ϕ.(o-t).w5.f'(h1).i1 <br>\n",
    "w2 = w2 - ϕ* ∂MSE/∂W2 = w2 - ϕ.(o−t)⋅W5⋅f'(h1)⋅i2 <br>\n",
    "w3 = w3 - ϕ* ∂MSE/∂W3 = w3 - ϕ.(o−t)⋅W6⋅f'(h2)⋅i1 <br>\n",
    "w4 = w4 - ϕ* ∂MSE/∂W4 = w4 - ϕ.(o−t)⋅W6⋅f'(h2)⋅i2 <br>\n",
    "w5 = w5 - ϕ* ∂MSE/∂W5 = w5 - ϕ.(o−t)⋅a1 <br>\n",
    "w6 = w6 - ϕ* ∂MSE/∂W6 = w6 - ϕ.(o−t)⋅a2 <br>\n",
    "\n",
    "**5. Assuming sigmoid activation function for hidden layer, updated weights are:**\n",
    "\n",
    "w1 = w1 - ϕ.(o-t).w5.a1.(1-a1).i1 <br>\n",
    "w2 = w2 - ϕ.(o−t)⋅W5⋅a1.(1-a1)⋅i2 <br>\n",
    "w3 = w3 - ϕ.(o−t)⋅W6⋅a2.(1-a2)⋅i1 <br>\n",
    "w4 = w4 - ϕ.(o−t)⋅W6⋅a2.(1-a2)⋅i2 <br>\n",
    "w5 = w5 - ϕ.(o−t)⋅a1 <br>\n",
    "w6 = w6 - ϕ.(o−t)⋅a2 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091ae25",
   "metadata": {},
   "source": [
    "#### 3.Assuming the true output to be 1, learning rate = 0.05 and MSE loss, calculate the new  weights using gradient descent (20 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4dbd132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New weights:\n",
      "W1: 0.10229211955061213\n",
      "W2: 0.27458423910122426\n",
      "W3: 0.11254075009017926\n",
      "W4: 0.15508150018035852\n",
      "W5: 0.2092667993947158\n",
      "W6: 0.18596725861606783\n"
     ]
    }
   ],
   "source": [
    "# Given true output\n",
    "t = 1\n",
    "# Learning rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Calculate the gradients\n",
    "grad_W5 = (out - t) * a1\n",
    "grad_W6 = (out - t) * a2\n",
    "grad_W1 = (out - t) * W5 * a1 * (1 - a1) * i1\n",
    "grad_W2 = (out - t) * W5 * a1 * (1 - a1) * i2\n",
    "grad_W3 = (out - t) * W6 * a2 * (1 - a2) * i1\n",
    "grad_W4 = (out - t) * W6 * a2 * (1 - a2) * i2\n",
    "\n",
    "# Update the weights using gradient descent\n",
    "W1 -= learning_rate * grad_W1\n",
    "W2 -= learning_rate * grad_W2\n",
    "W3 -= learning_rate * grad_W3\n",
    "W4 -= learning_rate * grad_W4\n",
    "W5 -= learning_rate * grad_W5\n",
    "W6 -= learning_rate * grad_W6\n",
    "\n",
    "print(\"New weights:\")\n",
    "print(\"W1:\", W1)\n",
    "print(\"W2:\", W2)\n",
    "print(\"W3:\", W3)\n",
    "print(\"W4:\", W4)\n",
    "print(\"W5:\", W5)\n",
    "print(\"W6:\", W6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08671ee0",
   "metadata": {},
   "source": [
    "#### 4.Implement another Forward Propagation with “Relu” activation function applied to neurons representing “h1” and “h2”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29edc9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with ReLU activation function applied to h1 and h2: 0.3616\n"
     ]
    }
   ],
   "source": [
    "# Given initial weights\n",
    "W1 = 0.10\n",
    "W2 = 0.27\n",
    "W3 = 0.11\n",
    "W4 = 0.15\n",
    "W5 = 0.18\n",
    "W6 = 0.16\n",
    "\n",
    "# Given inputs\n",
    "i1 = 2\n",
    "i2 = 4\n",
    "\n",
    "# Forward propagation with ReLU activation function\n",
    "h1 = max(0, i1 * W1 + i2 * W2)\n",
    "h2 = max(0, i1 * W3 + i2 * W4)\n",
    "\n",
    "\n",
    "\n",
    "a1 = h1\n",
    "a2 = h2\n",
    "\n",
    "out = a1 * W5 + a2 * W6\n",
    "print(\"Output with ReLU activation function applied to h1 and h2:\", out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
